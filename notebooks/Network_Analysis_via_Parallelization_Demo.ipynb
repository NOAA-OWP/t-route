{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riA1WwTTID3q"
   },
   "source": [
    "## NHD Network Analysis Demo\n",
    "___TL;DR___: *We are trying to parallelize hydraulic calculations for dynamic subsets of the U.S. river and stream network*<br><br>\n",
    "The following was developed as part of the process of preparing a method for forecasting flows on the US network of rivers and streams as represented in the National Hydrography Dataset (NHD). The NHD is a continuously evolving characterization of a fractal system so we felt that we needed to plan to have some flexibility. We hope to identify the complexity inherent in the network at different levels of resolution and we hope to be able to do so dynamically. The goal is also to be able to manage the complexity calculation for arbitrary collections of headwater points, such as might be obtained from a list of named streams or during a major flood event in a particular region.<br>\n",
    "As a point of terminology, we use the word 'routing' as shorthand to refer to the computation of the translation of a particular flow condition, high or low, to downstream (or in some cases upstream) areas of influence.\n",
    "The network complexity is related to the potential for parallelization of a serial analysis of the network. We have identified three levels of parallelization that may be implemented: \n",
    "1. System-level parallelization of independent networks -- the routing computations for the Mississippi River have little (nothing, except conceptual similarity and a shared existence on earth) to do with the computations for the Columbia river for any practical level of analysis. The system of networks across the US is what we are considering in general.\n",
    "1. Network-level parallelization of interconnected reaches -- There is a need to consider the computations for adjacent branches within a network of con-flowing streams, but with proper ordering, some of the computations may be considered in parallel. For example, the Illinois River headwaters and the Mississippi River headwaters are related within their broader Mississippi network, but the routing calculations for those headwaters are pratically agnostic to one another.\n",
    "1. Reach-level parallelization of the specific routing computation -- the numerical work of routing water downstream is a matrix computation and consists of exploring solutions to differential equations, all of which may potentially be examined in parallel, under the proper conditions and with suitable assumptions.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qv_SVt3VzC9J"
   },
   "source": [
    "### Import the git repo including test data\n",
    "This git repo is a fork/branch of the national water model public repository hosted by NCAR. The NCAR repo is the basis for the WRF-Hydro model that is presently the modeling engine of the [US National Water Model.](https://water.noaa.gov/about/nwm)<br>\n",
    "\n",
    "The network analysis code assumes that the downstream neighbor is identified in the table for each stream segment as is the case for the test datasets. \n",
    "\n",
    "We recognize that others have done similar work and may possibly have done it better. We are working on being more able to nimbly respond to suggestions and opportunities for improvement. Please let us know if you see something we could do better or of course feel free to fork and improve what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "0RBaWmyY7Z88",
    "outputId": "5e88d24d-9547-4347-eb77-509384696c2b"
   },
   "outputs": [],
   "source": [
    "import sys\n",
<<<<<<< HEAD
    "try:\n",
    "    import google.colab\n",
    "    ENV_IS_CL = True\n",
    "    !git clone --single-branch --branch master https://github.com/NOAA-OWP/t-route.git\n",
    "    sys.path.append('/content/src/python_framework')\n",
    "    !pip install geopandas\n",
    "    !pip install netcdf4\n",
    "    #default recursion limit (~1000) is slightly too small for the deepest branches of the network\n",
    "    sys.setrecursionlimit(6000) \n",
    "    #TODO: convert recursive functions to stack-based functions\n",
    "except:\n",
    "    ENV_IS_CL = False\n",
    "    sys.path.append(r'../src/python_framework')\n"
=======
    "\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    ENV_IS_CL = True\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"git\",\n",
    "            \"clone\",\n",
    "            \"--single-branch\",\n",
    "            \"--branch\",\n",
    "            \"master\",\n",
    "            \"https://github.com/NOAA-OWP/t-route.git\",\n",
    "        ]\n",
    "    )\n",
    "    sys.path.append(\"/content/src/python_framework_v01\")\n",
    "    subprocess.run([\"pip\", \"install\", \"geopandas\"])\n",
    "    subprocess.run([\"pip\", \"install\", \"netcdf4\"])\n",
    "    # default recursion limit (~1000) is slightly too small for the deepest branches of the network\n",
    "    sys.setrecursionlimit(6000)\n",
    "    # TODO: convert recursive functions to stack-based functions\n",
    "except:\n",
    "    ENV_IS_CL = False\n",
    "    sys.path.append(r\"../src/python_framework_v01\")\n"
>>>>>>> upstream/master
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJ0UirjC0iN1"
   },
   "source": [
    "### Some general functions and a test case\n",
    "The next blocks use functions for several modules (`nhd_network_utilities` and `recursive_print`) from the git repository, to create the `connnections` objects which characterize the network to be analyzed.<br><br>\n",
    "The `test_rows` object simulates a river network dataset such as we recieve from the National Hydrography Dataset. Each data row has a node ID, a 'to' node ID, and some other relevant data. For this test dataset, the second data column is a dummy length (and the last column could be some other value, but we haven't tried anything yet... stay tuned) and in our traversals, we can add up the lengths as a surrogate for more complex water routing functions we need to eventually manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KFnY-vZDYPue",
    "outputId": "58c2d71e-59d2-4d4e-818b-2dea1a337471"
   },
   "outputs": [],
   "source": [
    "import nhd_network_utilities as nnu\n",
    "import recursive_print\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> upstream/master
    "# def main():\n",
    "if 1 == 1:\n",
    "    \"\"\"##TEST\"\"\"\n",
    "    print(\"\")\n",
<<<<<<< HEAD
    "    print ('Executing Test')\n",
    "    # Test data\n",
    "    test_rows = [\n",
    "        [50,178,51,0],\n",
    "        [51,178,50,0],\n",
    "        [60,178,61,0],\n",
    "        [61,178,62,0],\n",
    "        [62,178,60,0],\n",
    "        [70,178,71,0],\n",
    "        [71,178,72,0],\n",
    "        [72,178,73,0],\n",
    "        [73,178,70,0],\n",
    "        [80,178,81,0],\n",
    "        [81,178,82,0],\n",
    "        [82,178,83,0],\n",
    "        [83,178,84,0],\n",
    "        [84,178,80,0],\n",
    "        [0,456,-999,0],\n",
    "        [1,178,4,0],\n",
    "        [2,394,0,0],\n",
    "        [3,301,2,0],\n",
    "        [4,798,0,0],\n",
    "        [5,679,4,0],\n",
    "        [6,523,0,0],\n",
    "        [7,815,2,0],\n",
    "        [8,841,-999,0],\n",
    "        [9,514,8,0],\n",
    "        [10,458,9,0],\n",
    "        [11,832,10,0],\n",
    "        [12,543,11,0],\n",
    "        [13,240,12,0],\n",
    "        [14,548,13,0],\n",
    "        [15,920,14,0],\n",
    "        [16,920,15,0],\n",
    "        [17,514,16,0],\n",
    "        [18,458,17,0],\n",
    "        [19,832,18,0],\n",
    "        [20,543,19,0],\n",
    "        [21,240,16,0],\n",
    "        [22,548,21,0],\n",
    "        [23,920,22,0],\n",
    "        [24,240,23,0],\n",
    "        [25,548,12,0],\n",
    "        [26,920,25,0],\n",
    "        [27,920,26,0],\n",
    "        [28,920,27,0],\n",
=======
    "    print(\"Executing Test\")\n",
    "    # Test data\n",
    "    test_rows = [\n",
    "        [50, 178, 51, 0],\n",
    "        [51, 178, 50, 0],\n",
    "        [60, 178, 61, 0],\n",
    "        [61, 178, 62, 0],\n",
    "        [62, 178, 60, 0],\n",
    "        [70, 178, 71, 0],\n",
    "        [71, 178, 72, 0],\n",
    "        [72, 178, 73, 0],\n",
    "        [73, 178, 70, 0],\n",
    "        [80, 178, 81, 0],\n",
    "        [81, 178, 82, 0],\n",
    "        [82, 178, 83, 0],\n",
    "        [83, 178, 84, 0],\n",
    "        [84, 178, 80, 0],\n",
    "        [0, 456, -999, 0],\n",
    "        [1, 178, 4, 0],\n",
    "        [2, 394, 0, 0],\n",
    "        [3, 301, 2, 0],\n",
    "        [4, 798, 0, 0],\n",
    "        [5, 679, 4, 0],\n",
    "        [6, 523, 0, 0],\n",
    "        [7, 815, 2, 0],\n",
    "        [8, 841, -999, 0],\n",
    "        [9, 514, 8, 0],\n",
    "        [10, 458, 9, 0],\n",
    "        [11, 832, 10, 0],\n",
    "        [12, 543, 11, 0],\n",
    "        [13, 240, 12, 0],\n",
    "        [14, 548, 13, 0],\n",
    "        [15, 920, 14, 0],\n",
    "        [16, 920, 15, 0],\n",
    "        [17, 514, 16, 0],\n",
    "        [18, 458, 17, 0],\n",
    "        [19, 832, 18, 0],\n",
    "        [20, 543, 19, 0],\n",
    "        [21, 240, 16, 0],\n",
    "        [22, 548, 21, 0],\n",
    "        [23, 920, 22, 0],\n",
    "        [24, 240, 23, 0],\n",
    "        [25, 548, 12, 0],\n",
    "        [26, 920, 25, 0],\n",
    "        [27, 920, 26, 0],\n",
    "        [28, 920, 27, 0],\n",
>>>>>>> upstream/master
    "    ]\n",
    "\n",
    "    test_key_col = 0\n",
    "    test_downstream_col = 2\n",
    "    test_length_col = 1\n",
    "    test_terminal_code = -999\n",
    "    debuglevel = 0\n",
    "    verbose = True\n",
    "\n",
    "    test_return_values = nnu.build_connections_object(\n",
<<<<<<< HEAD
    "        geo_file_rows = test_rows\n",
    "                , key_col = test_key_col\n",
    "                , mask_set = {row[test_key_col] for row in test_rows}\n",
    "                , downstream_col = test_downstream_col\n",
    "                , length_col = test_length_col\n",
    "                , terminal_code = test_terminal_code\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel\n",
    "    \n",
    "    )\n",
    "\n",
    "    recursive_print.print_connections(\n",
    "                headwater_keys = test_return_values[3]\n",
    "                , down_connections = test_return_values[0]\n",
    "                , up_connections = test_return_values[0]\n",
    "                , terminal_code = test_terminal_code\n",
    "                , terminal_keys = test_return_values[4]\n",
    "                , terminal_ref_keys = test_return_values[5]\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    recursive_print.print_basic_network_info(\n",
    "                connections = test_return_values[0]\n",
    "                , headwater_keys = test_return_values[3]\n",
    "                , junction_keys = test_return_values[7]\n",
    "                , terminal_keys = test_return_values[4]\n",
    "                , terminal_code = test_terminal_code\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
=======
    "        geo_file_rows=test_rows,\n",
    "        key_col=test_key_col,\n",
    "        mask_set={row[test_key_col] for row in test_rows},\n",
    "        downstream_col=test_downstream_col,\n",
    "        length_col=test_length_col,\n",
    "        terminal_code=test_terminal_code,\n",
    "        verbose=verbose,\n",
    "        debuglevel=debuglevel,\n",
    "    )\n",
    "\n",
    "    recursive_print.print_connections(\n",
    "        headwater_keys=test_return_values[3],\n",
    "        down_connections=test_return_values[0],\n",
    "        up_connections=test_return_values[0],\n",
    "        terminal_code=test_terminal_code,\n",
    "        terminal_keys=test_return_values[4],\n",
    "        terminal_ref_keys=test_return_values[5],\n",
    "        debuglevel=debuglevel,\n",
    "    )\n",
    "\n",
    "    recursive_print.print_basic_network_info(\n",
    "        connections=test_return_values[0],\n",
    "        headwater_keys=test_return_values[3],\n",
    "        junction_keys=test_return_values[7],\n",
    "        terminal_keys=test_return_values[4],\n",
    "        terminal_code=test_terminal_code,\n",
    "        verbose=verbose,\n",
    "        debuglevel=debuglevel,\n",
    "    )\n",
>>>>>>> upstream/master
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5xhb-Q4pCbX"
   },
   "source": [
    "### Real Networks\n",
    "(you can skip this cell to test the code on the simple case generated above...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "il50SP8eZBk2",
    "outputId": "32c43141-2c53-4919-b1e6-a81dd9f6dc92"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
<<<<<<< HEAD
    "if ENV_IS_CL: root = '/content/wrf_hydro_nwm_public/trunk/NDHMS/dynamic_channel_routing/'\n",
    "elif not ENV_IS_CL: root = os.path.dirname(os.path.abspath(''))\n",
    "test_folder = os.path.join(root, r'test')\n",
    "geo_input_folder = os.path.join(test_folder, r'input', r'geo')\n",
    "\n",
    "supernetworks = {}\n",
    "\"\"\"##NHD Subset (Brazos/Lower Colorado)\"\"\"\n",
    "supernetworks.update({'Brazos_LowerColorado_ge5':{'data':None,'values':None}})\n",
    "\"\"\"##NHD CONUS order 5 and greater\"\"\"\n",
    "supernetworks.update({'CONUS_ge5':{'data':None,'values':None}})\n",
    "\"\"\"These are large -- be careful\"\"\"\n",
    "supernetworks.update({'CONUS_FULL_RES_v20':{'data':None,'values':None}})\n",
    "supernetworks.update({'CONUS_Named_Streams':{'data':None,'values':None}}) #create a subset of the full resolution by reading the GNIS field\n",
=======
    "if ENV_IS_CL:\n",
    "    root = \"/content/t-route-jsh/\"\n",
    "elif not ENV_IS_CL:\n",
    "    root = os.path.dirname(os.path.abspath(\"\"))\n",
    "test_folder = os.path.join(root, r\"test\")\n",
    "geo_input_folder = os.path.join(test_folder, r\"input\", r\"geo\")\n",
    "\n",
    "supernetworks = {}\n",
    "\"\"\"##NHD Subset (Brazos/Lower Colorado)\"\"\"\n",
    "supernetworks.update({\"Brazos_LowerColorado_ge5\": {\"data\": None, \"values\": None}})\n",
    "\"\"\"##NHD CONUS order 5 and greater\"\"\"\n",
    "supernetworks.update({\"CONUS_ge5\": {\"data\": None, \"values\": None}})\n",
    "\"\"\"These are large -- be careful\"\"\"\n",
    "supernetworks.update({\"CONUS_FULL_RES_v20\": {\"data\": None, \"values\": None}})\n",
    "supernetworks.update(\n",
    "    {\"CONUS_Named_Streams\": {\"data\": None, \"values\": None}}\n",
    ")  # create a subset of the full resolution by reading the GNIS field\n",
>>>>>>> upstream/master
    "\n",
    "debuglevel = -1\n",
    "verbose = True\n",
    "\n",
    "# The CONUS_ge5 and Brazos_LowerColorado_ge5 datasets are included\n",
    "# in the github test folder and are extracts from the NHD version 1.2 datasets\n",
    "# from https://www.nohrsc.noaa.gov/pub/staff/keicher/NWM_live/web/data_tools/\n",
<<<<<<< HEAD
    "#  \n",
=======
    "#\n",
>>>>>>> upstream/master
    "# The CONUS_FULL_RES file was generated from the RouteLink file in the parameter\n",
    "# archive and converted to a compressed NetCDF via the following command:\n",
    "# nccopy -d1 -s RouteLink_NWMv2.0_20190517_cheyenne_pull.nc RouteLink_NWMv2.0_20190517_cheyenne_pull.nc4s\n",
    "# TODO: Explain CONUS_Named_Streams\n",
    "# CONUS_Named_Streams was generated by intersecting the FULL_RES file ...\n",
<<<<<<< HEAD
    "# of the data in the nohrsc-hosted archive but are too large to efficiently \n",
    "# package inside of the repository. \n",
    "\n",
    "for sn in supernetworks:\n",
    "    print(sn)\n",
    "    supernetworks[sn]['data'], supernetworks[sn]['values'] \\\n",
    "    = nnu.set_networks(supernetwork = sn\n",
    "      , geo_input_folder = geo_input_folder\n",
    "      , verbose = verbose\n",
    "      , debuglevel = debuglevel)\n"
=======
    "# of the data in the nohrsc-hosted archive but are too large to efficiently\n",
    "# package inside of the repository.\n",
    "\n",
    "for sn in supernetworks:\n",
    "    print(sn)\n",
    "    supernetworks[sn][\"data\"], supernetworks[sn][\"values\"] = nnu.set_networks(\n",
    "        supernetwork=sn,\n",
    "        geo_input_folder=geo_input_folder,\n",
    "        verbose=verbose,\n",
    "        debuglevel=debuglevel,\n",
    "    )\n"
>>>>>>> upstream/master
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fuRiEXFsvOK"
   },
   "source": [
    "### With this, we can separate the different rivers in the network\n",
    "Once a 'connection' object has been created with a representation of the river network, we can traverse that object and perform calculations -- in the example below, we parallelize the process of traversing the independent portions of the network and then serially compute the number of junctions. This corresponds to the \"**System-level parallelization**\" mentioned as _item 1_ above.\n",
    "### NOW for the next step\n",
    "We could compute total upstream length or (and this is the real goal) flow due to incoming lateral contributions from the land accumulated over the entire upstream network. That second calculation can also be parallelized but we have to figure out how to accomplish intelligently so that the collective calculation is network-aware. Such a parallelization would be the \"**Network-level parallelization of interconnected reaches**\" mentioned as _item 2_ above. The upstream length will depend on the number of upstream branches and their configuration, so there has to be some concept of stream order and topology built into the parallelization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "\n",
    "#parallel compute\n",
=======
    "# parallel compute\n",
>>>>>>> upstream/master
    "import time\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
<<<<<<< HEAD
    "'''Test'''\n",
=======
    "\"\"\"Test\"\"\"\n",
>>>>>>> upstream/master
    "# terminal_keys = test_return_values[4]\n",
    "# circular_keys = test_return_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = test_return_values[0]\n",
    "# terminal_code = test_terminal_code\n",
<<<<<<< HEAD
    "'''Streams of NHD order 5 or greater confluent to the Brazos and Lower Colorado'''\n",
=======
    "\"\"\"Streams of NHD order 5 or greater confluent to the Brazos and Lower Colorado\"\"\"\n",
>>>>>>> upstream/master
    "# terminal_keys = supernetworks['Brazos_LowerColorado_ge5']['values'][4]\n",
    "# circular_keys = supernetworks['Brazos_LowerColorado_ge5']['values'][6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = supernetworks['Brazos_LowerColorado_ge5']['values'][0]\n",
    "# terminal_code = supernetworks['Brazos_LowerColorado_ge5']['data']['terminal_code']\n",
<<<<<<< HEAD
    "'''CONUS streams of NHD order 5 or greater'''\n",
=======
    "\"\"\"CONUS streams of NHD order 5 or greater\"\"\"\n",
>>>>>>> upstream/master
    "# terminal_keys = supernetworks['CONUS_ge5']['values'][4]\n",
    "# circular_keys = supernetworks['CONUS_ge5']['values'][6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = supernetworks['CONUS_ge5']['values'][0]\n",
    "# terminal_code = supernetworks['CONUS_ge5']['data']['terminal_code']\n",
<<<<<<< HEAD
    "'''Full Res NHD'''\n",
    "terminal_keys = supernetworks['CONUS_FULL_RES_v20']['values'][4] \n",
    "circular_keys = supernetworks['CONUS_FULL_RES_v20']['values'][6]\n",
    "terminal_keys_super = terminal_keys - circular_keys\n",
    "con = supernetworks['CONUS_FULL_RES_v20']['values'][0]\n",
    "terminal_code = supernetworks['CONUS_FULL_RES_v20']['data']['terminal_code']\n",
    "'''Named Streams'''\n",
=======
    "\"\"\"Full Res NHD\"\"\"\n",
    "terminal_keys = supernetworks[\"CONUS_FULL_RES_v20\"][\"values\"][4]\n",
    "circular_keys = supernetworks[\"CONUS_FULL_RES_v20\"][\"values\"][6]\n",
    "terminal_keys_super = terminal_keys - circular_keys\n",
    "con = supernetworks[\"CONUS_FULL_RES_v20\"][\"values\"][0]\n",
    "terminal_code = supernetworks[\"CONUS_FULL_RES_v20\"][\"data\"][\"terminal_code\"]\n",
    "\"\"\"Named Streams\"\"\"\n",
>>>>>>> upstream/master
    "# terminal_keys = supernetworks['CONUS_Named_Streams']['values'][4]\n",
    "# circular_keys = supernetworks['CONUS_Named_Streams']['values'][6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = supernetworks['CONUS_Named_Streams']['values'][0]\n",
    "# terminal_code = supernetworks['CONUS_Named_Streams']['data']['terminal_code']\n",
    "\n",
<<<<<<< HEAD
    "def recursive_junction_read (\n",
    "                             keys\n",
    "                             , network\n",
    "                             , terminal_code = 0\n",
    "                             , verbose = False\n",
    "                             , debuglevel = 0\n",
    "                            ):\n",
=======
    "\n",
    "def recursive_junction_read(\n",
    "    keys, network, terminal_code=0, verbose=False, debuglevel=0\n",
    "):\n",
>>>>>>> upstream/master
    "    global con\n",
    "    for key in keys:\n",
    "        ckey = key\n",
    "        try:\n",
<<<<<<< HEAD
    "            ukeys = con[key]['upstreams']\n",
    "            while not len(ukeys) >= 2 and not (ukeys == {terminal_code}):\n",
    "                if debuglevel <= -3: print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "                # the terminal code will indicate a headwater\n",
    "                if debuglevel <= -4: print(ukeys)\n",
    "                (ckey,) = ukeys\n",
    "                ukeys = con[ckey]['upstreams']\n",
    "            if ukeys == {terminal_code}:\n",
    "                if debuglevel <= -3: print(f\"headwater found at {ckey}\")\n",
    "                network['segment_count'] += 1\n",
    "                if debuglevel <= -3: print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "            elif len(ukeys) >= 2:\n",
    "                network['segment_count'] += 1\n",
    "                if debuglevel <= -3: print(f\"junction found at {ckey} with upstreams {ukeys}\")\n",
    "                network['segment_count'] += 1\n",
    "                if debuglevel <= -3: print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "                network['junction_count'] += 1 #the Terminal Segment\n",
    "                recursive_junction_read (ukeys, network, terminal_code = terminal_code, verbose = verbose, debuglevel = debuglevel) \n",
    "                # print(ukeys)\n",
    "                ukeys = con[ckey]['upstreams']\n",
    "                ckey = ukeys\n",
    "        except:\n",
    "            if debuglevel <= -2: \n",
    "                print(f'There is a problem with connection: {key}: {con[key]}')\n",
    "\n",
    "def network_trace(\n",
    "                        nid\n",
    "                        , terminal_code = terminal_code\n",
    "                        , verbose= False\n",
    "                        , debuglevel = 0\n",
    "                        ):\n",
=======
    "            ukeys = con[key][\"upstreams\"]\n",
    "            while not len(ukeys) >= 2 and not (ukeys == {terminal_code}):\n",
    "                if debuglevel <= -3:\n",
    "                    print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "                # the terminal code will indicate a headwater\n",
    "                if debuglevel <= -4:\n",
    "                    print(ukeys)\n",
    "                (ckey,) = ukeys\n",
    "                ukeys = con[ckey][\"upstreams\"]\n",
    "            if ukeys == {terminal_code}:\n",
    "                if debuglevel <= -3:\n",
    "                    print(f\"headwater found at {ckey}\")\n",
    "                network[\"segment_count\"] += 1\n",
    "                if debuglevel <= -3:\n",
    "                    print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "            elif len(ukeys) >= 2:\n",
    "                network[\"segment_count\"] += 1\n",
    "                if debuglevel <= -3:\n",
    "                    print(f\"junction found at {ckey} with upstreams {ukeys}\")\n",
    "                network[\"segment_count\"] += 1\n",
    "                if debuglevel <= -3:\n",
    "                    print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "                network[\"junction_count\"] += 1  # the Terminal Segment\n",
    "                recursive_junction_read(\n",
    "                    ukeys,\n",
    "                    network,\n",
    "                    terminal_code=terminal_code,\n",
    "                    verbose=verbose,\n",
    "                    debuglevel=debuglevel,\n",
    "                )\n",
    "                # print(ukeys)\n",
    "                ukeys = con[ckey][\"upstreams\"]\n",
    "                ckey = ukeys\n",
    "        except:\n",
    "            if debuglevel <= -2:\n",
    "                print(f\"There is a problem with connection: {key}: {con[key]}\")\n",
    "\n",
    "\n",
    "def network_trace(nid, terminal_code=terminal_code, verbose=False, debuglevel=0):\n",
>>>>>>> upstream/master
    "\n",
    "    network = {}\n",
    "    global con\n",
    "    us_length_total = 0\n",
<<<<<<< HEAD
    "    \n",
    "    if verbose: print(f'\\ntraversing upstream on network {nid}:')\n",
    "    # try:\n",
    "    if 1 == 1:\n",
    "        network.update({'junction_count': 0})\n",
    "        network.update({'segment_count': 0}) #the Terminal Segment\n",
    "        recursive_junction_read([nid], network, verbose = verbose, terminal_code = terminal_code, debuglevel = debuglevel)\n",
    "        if verbose: print(f\"junctions: {network['junction_count']}\")\n",
    "        if verbose: print(f\"segments: {network['segment_count']}\")\n",
    "    # except Exception as exc:\n",
    "    #     print(exc)\n",
    "    #TODO: compute upstream length as a surrogate for the routing computation\n",
    "    return {nid: network, 'upstream_length': us_length_total}\n",
    "\n"
=======
    "\n",
    "    if verbose:\n",
    "        print(f\"\\ntraversing upstream on network {nid}:\")\n",
    "    # try:\n",
    "    if 1 == 1:\n",
    "        network.update({\"junction_count\": 0})\n",
    "        network.update({\"segment_count\": 0})  # the Terminal Segment\n",
    "        recursive_junction_read(\n",
    "            [nid],\n",
    "            network,\n",
    "            verbose=verbose,\n",
    "            terminal_code=terminal_code,\n",
    "            debuglevel=debuglevel,\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"junctions: {network['junction_count']}\")\n",
    "        if verbose:\n",
    "            print(f\"segments: {network['segment_count']}\")\n",
    "    # except Exception as exc:\n",
    "    #     print(exc)\n",
    "    # TODO: compute upstream length as a surrogate for the routing computation\n",
    "    return {nid: network, \"upstream_length\": us_length_total}\n"
>>>>>>> upstream/master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Gz15rLB9WnBl",
    "outputId": "b872a326-258a-46f8-e9d6-66e0772a7957"
   },
   "outputs": [],
   "source": [
    "###continuing from previous cell\n",
<<<<<<< HEAD
    "networks = {terminal_key:{}\n",
    "                  for terminal_key in terminal_keys_super \n",
    "# # Toggle these comments to try removing/isolating some of the larger networks\n",
    "#                   if terminal_key in [\n",
    "#                   if terminal_key not in [\n",
    "#                           22811611 # Mississippi River, LA\n",
    "#                           , 23832907 # Columbia River, WA\n",
    "#                           , 626220 # Rio Grande, TX/MX\n",
    "#                           , 21412883 # Colorado River, AZ/MX\n",
    "#                           , 18524217 # Mobile River, AL\n",
    "#                           , 15183793 # Atchafalaya, LA\n",
    "#                   ]\n",
    "                 }  \n",
    "debuglevel = -1\n",
    "verbose = False\n",
    "\n",
    "if verbose: print('verbose output')\n",
    "if verbose: print(f'number of Independent Networks to be analyzed is {len(super_networks)}')\n",
    "if verbose: print(f'Multi-processing will use {multiprocessing.cpu_count()} CPUs')\n",
    "if verbose: print(f'debuglevel is {debuglevel}')\n",
=======
    "networks = {\n",
    "    terminal_key: {}\n",
    "    for terminal_key in terminal_keys_super\n",
    "    # # Toggle these comments to try removing/isolating some of the larger networks\n",
    "    #                   if terminal_key in [\n",
    "    #                   if terminal_key not in [\n",
    "    #                           22811611 # Mississippi River, LA\n",
    "    #                           , 23832907 # Columbia River, WA\n",
    "    #                           , 626220 # Rio Grande, TX/MX\n",
    "    #                           , 21412883 # Colorado River, AZ/MX\n",
    "    #                           , 18524217 # Mobile River, AL\n",
    "    #                           , 15183793 # Atchafalaya, LA\n",
    "    #                   ]\n",
    "}\n",
    "debuglevel = -1\n",
    "verbose = False\n",
    "\n",
    "if verbose:\n",
    "    print(\"verbose output\")\n",
    "if verbose:\n",
    "    print(f\"number of Independent Networks to be analyzed is {len(super_networks)}\")\n",
    "if verbose:\n",
    "    print(f\"Multi-processing will use {multiprocessing.cpu_count()} CPUs\")\n",
    "if verbose:\n",
    "    print(f\"debuglevel is {debuglevel}\")\n",
>>>>>>> upstream/master
    "\n",
    "start_time = time.time()\n",
    "results_serial = {}\n",
    "for nid, network in networks.items():\n",
<<<<<<< HEAD
    "    network.update(network_trace(nid, terminal_code = terminal_code, verbose = verbose, debuglevel = debuglevel)[nid])\n",
    "print(\"--- %s seconds: serial compute ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1: print(len(networks.items()))\n",
    "if debuglevel <= -2: print(networks)\n",
    "\n",
    "    ## Notice that I'm not timing the initialization in each case, \n",
    "    ## which might be considered cheating a little bit. \n",
    "    ## I timed it for the first case for reference.\n",
    "nids = (nid for nid in networks)\n",
    "start_time = time.time()\n",
    "with multiprocessing.Pool() as pool:\n",
    "    print(\"--- %s seconds: parallel overhead to load multiprocessing.Pool() ---\" % (time.time() - start_time))    \n",
    "    start_time = time.time()\n",
    "    results = pool.map(network_trace, nids)\n",
    "    print(\"--- %s seconds: parallel compute using default terminal_code ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1: print(len(results))\n",
    "if debuglevel <= -2: print(results)\n",
    "\n",
    "nids = (nid for nid in networks)\n",
    "snt = partial(network_trace, terminal_code = terminal_code)\n",
    "with multiprocessing.Pool() as pool:\n",
    "    start_time = time.time()\n",
    "    results = pool.map(snt, nids)\n",
    "    print(\"--- %s seconds: parallel compute using partial function ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1: print(len(results))\n",
    "if debuglevel <= -2: print(results)\n",
    "\n",
    "nidsWtc = ([nid,terminal_code] for nid in networks)\n",
    "with multiprocessing.Pool() as pool:\n",
    "    start_time = time.time()\n",
    "    results = pool.starmap(network_trace, nidsWtc)\n",
    "    print(\"--- %s seconds: parallel compute with list of lists and starmap ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1: print(len(results))\n",
    "if debuglevel <= -2: print(results)"
=======
    "    network.update(\n",
    "        network_trace(\n",
    "            nid, terminal_code=terminal_code, verbose=verbose, debuglevel=debuglevel\n",
    "        )[nid]\n",
    "    )\n",
    "print(\"--- %s seconds: serial compute ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1:\n",
    "    print(len(networks.items()))\n",
    "if debuglevel <= -2:\n",
    "    print(networks)\n",
    "\n",
    "## Notice that I'm not timing the initialization in each case,\n",
    "## which might be considered cheating a little bit.\n",
    "## I timed it for the first case for reference.\n",
    "nids = (nid for nid in networks)\n",
    "start_time = time.time()\n",
    "with multiprocessing.Pool() as pool:\n",
    "    print(\n",
    "        \"--- %s seconds: parallel overhead to load multiprocessing.Pool() ---\"\n",
    "        % (time.time() - start_time)\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    results = pool.map(network_trace, nids)\n",
    "    print(\n",
    "        \"--- %s seconds: parallel compute using default terminal_code ---\"\n",
    "        % (time.time() - start_time)\n",
    "    )\n",
    "if debuglevel <= -1:\n",
    "    print(len(results))\n",
    "if debuglevel <= -2:\n",
    "    print(results)\n",
    "\n",
    "nids = (nid for nid in networks)\n",
    "snt = partial(network_trace, terminal_code=terminal_code)\n",
    "with multiprocessing.Pool() as pool:\n",
    "    start_time = time.time()\n",
    "    results = pool.map(snt, nids)\n",
    "    print(\n",
    "        \"--- %s seconds: parallel compute using partial function ---\"\n",
    "        % (time.time() - start_time)\n",
    "    )\n",
    "if debuglevel <= -1:\n",
    "    print(len(results))\n",
    "if debuglevel <= -2:\n",
    "    print(results)\n",
    "\n",
    "nidsWtc = ([nid, terminal_code] for nid in networks)\n",
    "with multiprocessing.Pool() as pool:\n",
    "    start_time = time.time()\n",
    "    results = pool.starmap(network_trace, nidsWtc)\n",
    "    print(\n",
    "        \"--- %s seconds: parallel compute with list of lists and starmap ---\"\n",
    "        % (time.time() - start_time)\n",
    "    )\n",
    "if debuglevel <= -1:\n",
    "    print(len(results))\n",
    "if debuglevel <= -2:\n",
    "    print(results)\n"
>>>>>>> upstream/master
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "text",
    "id": "pvfefJVLvxwe"
   },
   "source": [
    "### Colab output\n",
    "\n",
    "For the 14351 independent networks (and 2.7M segments) of the NWM Full Resolution dataset on a Google colaboratory VM, the algorithm steps through the segments of the different independent networks in order in about 4 seconds, regardless of the parallelization method. On a modest workstation at NWC with 12 cores, the compute time was approximately 2 seconds for a serial compute and 1.1 seconds for each of the parallel methods. Notably, because we are only breaking apart the independent networks, the maximum parallel speedup is limited by the size of the largest network -- the Mississippi River. By executing with the terminal node for the Mississippi River removed from the evaluation set, the serial calculation takes only 1.4 seconds and the parallel executions drop to approximately 0.4 seconds.<br>\n",
    "```\n",
    "--- 4.638037443161011 seconds: serial compute ---  \n",
    "14351  \n",
    "--- 0.0757453441619873 seconds: parallel overhead to load multiprocessing.Pool() ---  \n",
    "--- 4.344968557357788 seconds: parallel compute using default terminal_code ---  \n",
    "14351  \n",
    "--- 4.323424577713013 seconds: parallel compute using partial function ---  \n",
    "14351  \n",
    "--- 4.167566537857056 seconds: parallel compute with list of lists and starmap ---  \n",
    "14351  \n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Network Analysis via Parallelization Demo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.8"
=======
   "version": ""
>>>>>>> upstream/master
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
